{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjhz6VS6SMb"
      },
      "source": [
        "# Phishing Website Classification: A Lightweight Bayesian Approach\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "90% of security breaches are a result of phishing attacks, in which attackers harvest the personal information of unsuspecting users under false pretenses ([source](https://spanning.com/blog/cyberattacks-2021-phishing-ransomware-data-breach-statistics/#:~:text=According%20to%20CISCO's%202021,14%20malicious%20emails%20every%20year.)). There was a 74% increase in phishing attacks per second in the last year, making the prevention of phishing attacks more pressing than ever ([source](https://www.phishingbox.com/resources/phishing-facts)). Phishing attacks rely on phishing websites — sites constructed to mimic real websites and steal victims' personal information — to breach their targets. These sites often look very similar to the real versions of sites, even using identical images, fonts, and other static content, as well as impersonating their URLs. \n",
        "\n",
        "So how can we prevent phishing attacks? One option is that we stop phishing websites in their tracks, blocking them from loading in browsers before victims can give up their personal information. To implement a solution like this, we need to create a classification model, that given a website's features can classify it as either phishing or legitimate.\n",
        "\n",
        "In the following Colab, I will implement a lightweight naive Bayes classifier from scratch for phishing websites, using only the most impactful features. The dataset that I used for this project is from UC Irvine's Machine Learning Repository and can be found [here](https://archive.ics.uci.edu/ml/datasets/phishing+websites#). **Adding complexity to this challenge, most of the features were not binary.** Additionally, I implemented this model before Problem Set 6 came out, so I took a different approach, opting to use the pure argmax definition and log probabilities. Using only this simple probabilistic model and only the top 5 most impactful features (out of 30 total), I was able to **classify phishing websites with an accuracy consistently in the range of 91%-93%**. No fancy gradient descent required!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3PuWb7YAnItF",
        "outputId": "003245ef-bb8d-4554-d573-9d75ddebda2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('id', 'having_IP_Address', 'URL_Length', 'Shortining_Service', 'having_At_Symbol', 'double_slash_redirecting', 'Prefix_Suffix', 'having_Sub_Domain', 'SSLfinal_State', 'Domain_registeration_length', 'Favicon', 'port', 'HTTPS_token', 'Request_URL', 'URL_of_Anchor', 'Links_in_tags', 'SFH', 'Submitting_to_email', 'Abnormal_URL', 'Redirect', 'on_mouseover', 'RightClick', 'popUpWidnow', 'Iframe', 'age_of_domain', 'DNSRecord', 'web_traffic', 'Page_Rank', 'Google_Index', 'Links_pointing_to_page', 'Statistical_report', 'Result')\n",
            "[(1, -1, 1,  1, 1, -1, -1, -1, -1, -1, 1, 1, -1,  1, -1,  1, -1, -1, -1, 0,  1, 1,  1, 1, -1, -1, -1, -1, 1,  1, -1, -1)\n",
            " (2,  1, 1,  1, 1,  1, -1,  0,  1, -1, 1, 1, -1,  1,  0, -1, -1,  1,  1, 0,  1, 1,  1, 1, -1, -1,  0, -1, 1,  1,  1, -1)\n",
            " (3,  1, 0,  1, 1,  1, -1, -1, -1, -1, 1, 1, -1,  1,  0, -1, -1, -1, -1, 0,  1, 1,  1, 1,  1, -1,  1, -1, 1,  0, -1, -1)\n",
            " (4,  1, 0,  1, 1,  1, -1, -1, -1,  1, 1, 1, -1, -1,  0,  0, -1,  1,  1, 0,  1, 1,  1, 1, -1, -1,  1, -1, 1, -1,  1, -1)\n",
            " (5,  1, 0, -1, 1,  1, -1,  1,  1, -1, 1, 1,  1,  1,  0,  0, -1,  1,  1, 0, -1, 1, -1, 1, -1, -1,  0, -1, 1,  1,  1,  1)]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from math import log\n",
        "\n",
        "data_raw = np.genfromtxt(\n",
        "    \"./phishing-data.csv\",\n",
        "    delimiter=\",\",\n",
        "    dtype=int,\n",
        "    names=True,\n",
        ")\n",
        "\n",
        "# Split 30% of data off as test data, reserving 70% for training\n",
        "data_train, data_test = train_test_split(data_raw, test_size=0.3, random_state=7) # Feel free to change random state\n",
        "# Remove results column from test data\n",
        "data_test = data_test[:-1]\n",
        "print(data_raw.dtype.names)\n",
        "print(data_raw[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8jF2646Fj5c"
      },
      "source": [
        "First, we have to load our data, and separate our training and test sets. The 30 columns indicating features of each website and the first 5 websites in the dataset are printed above. Importantly, note that most of these features are not binary, with their supports discretely indicating the degree of suspiciousness from -1 to 1. In fact, it required some sleuthing and analysis to determine that 1 indicated a phishing website and -1 indicated legitimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "vE5gYJK8n3co"
      },
      "outputs": [],
      "source": [
        "def calc_feature_impact(threshold, n_features):\n",
        "    ratios = {}\n",
        "    for i in range(1, len(data_raw[0]) - 1):\n",
        "        feat_tot = phishing_feat = phishing_not_feat = 0\n",
        "        for site in data_raw:\n",
        "            if site[i] >= threshold:\n",
        "                feat_tot += 1\n",
        "                if site[\"Result\"] == 1:\n",
        "                    phishing_feat += 1\n",
        "            else:\n",
        "                if site[\"Result\"] == 1:\n",
        "                    phishing_not_feat += 1\n",
        "\n",
        "        # Use laplace smoothing to ensure no division by 0\n",
        "        # Each ratio[i] is P(Y = 1 | X_i >= threshold) / P(Y = 1 | X_i < threshold)\n",
        "        p_y1_given_xi_geq_thresh = ((phishing_feat + 1) / (feat_tot + 2))\n",
        "        p_y1_given_xi_lt_thresh = ((phishing_not_feat + 1) / (len(data_raw) - feat_tot + 2))\n",
        "        ratios[i] = p_y1_given_xi_geq_thresh / p_y1_given_xi_lt_thresh\n",
        "\n",
        "    # Sort ratios dictionary by value\n",
        "    sorted_ratios = sorted(ratios.items(), key=lambda x: x[1])\n",
        "    result = sorted_ratios[len(sorted_ratios) - n_features :]\n",
        "\n",
        "    print(\"\\nTop {} Most Impactful Features (threshold = {})\".format(n_features, threshold))\n",
        "    for i in result:\n",
        "        print(\n",
        "            \"P(phishing | {feat} >= {threshold}) / P(phishing | {feat} < {threshold}) = {val}\".format(\n",
        "                feat=data_raw.dtype.names[i[0]],\n",
        "                threshold=threshold,\n",
        "                val=i[1],\n",
        "            )\n",
        "        )\n",
        "    \n",
        "    return [data_raw.dtype.names[i[0]] for i in result]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_QG8EWiIHZd"
      },
      "source": [
        "My primary intention with this classifier was to make it as lightweight as possible while still being effective. I wanted to make it a simple Bayesian classifier, without any more complex machine learning. I also wanted to make the amount of computation required for training the model, as well as the amount of data required for the saved model, to be as minimal as possible. Therefore, my first step was to write up a function that could tell me what the most impactful variables would be so I could only include their probabilities. A good measure for feature impact in this context is to compare the conditional probabilities that a website is a phishing website given a feature's different values. For every feature $X_i$ in the dataset, this function calculates $\\frac{P(\\text{phishing} | X_i >= x)}{P(\\text{phishing} | X_i < x)}$ for some threshold $x$. It then returns the top *n_features* most impactful features. I ultimately decided on using a threshold of 0, meaning suspicious, because I wanted the features to be sorted by their impact given that the feature is rated as suspicious or greater. This, perhaps intuitively, aligned with the greatest model accuracy. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--oXCFsvo7Im",
        "outputId": "2b085cbe-7f12-417e-ab64-27ea86a48579"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Top 5 Most Impactful Features (threshold = 0)\n",
            "P(phishing | Request_URL >= 0) / P(phishing | Request_URL < 0) = 1.6325491434634134\n",
            "P(phishing | web_traffic >= 0) / P(phishing | web_traffic < 0) = 1.665135680769705\n",
            "P(phishing | Prefix_Suffix >= 0) / P(phishing | Prefix_Suffix < 0) = 2.0425019147721932\n",
            "P(phishing | SSLfinal_State >= 0) / P(phishing | SSLfinal_State < 0) = 5.290063905325444\n",
            "P(phishing | URL_of_Anchor >= 0) / P(phishing | URL_of_Anchor < 0) = 69.88667072216911\n"
          ]
        }
      ],
      "source": [
        "n_features = 5  # Feel free to try different values here!\n",
        "most_impact = calc_feature_impact(threshold=0, n_features=n_features) # Impact given a feature is at least suspicious"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "CowwTpgqpmgT"
      },
      "outputs": [],
      "source": [
        "def cond_prob(feat, feat_val, y, data):\n",
        "    y_tot = 0\n",
        "    feat_given_y = 0\n",
        "    for site in data:\n",
        "        if site[\"Result\"] == y:\n",
        "            y_tot += 1\n",
        "            if site[feat] == feat_val:\n",
        "                feat_given_y += 1\n",
        "\n",
        "  # P(X_i = x | Y = y) with laplace smoothing\n",
        "  # note: assuming at least one X_i = x and one X_i != x, regardless of support\n",
        "    return (feat_given_y + 1) / (y_tot + 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKBa8sQbeC0f"
      },
      "source": [
        "This function uses laplace smoothing to calculate $P(X_i=x | Y=y)$, for any feature $X_i$, any value of that feature $x$, and any value of $Y$. $Y$ is the random variable representing whether or not a website is legitimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Npl3Mb45pUPo"
      },
      "outputs": [],
      "source": [
        "def calc_support(X, data):\n",
        "    support = dict.fromkeys(X, set())\n",
        "    for site in data:\n",
        "        for X_i in X:\n",
        "            support[X_i].add(site[X_i])\n",
        "    return support"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-PA0Bwxe-XG"
      },
      "source": [
        "Because the features have varying supports, most of which not being binary, I created a function that would calculate the support of each feature. This allowed me to later reference the supports in order to calculate the conditional probability for each possible value of each feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Jf_DdIkKpGRF"
      },
      "outputs": [],
      "source": [
        "def calc_probs_given_y(X, data):\n",
        "    probs_given_y = {}\n",
        "    support = calc_support(X, data)\n",
        "    for X_i in X:\n",
        "        probs_given_y[X_i] = {str(val): (cond_prob(X_i, val, -1, data), cond_prob(X_i, val, 1, data)) for val in support[X_i]}\n",
        "    return probs_given_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVEkzzK9foJN"
      },
      "source": [
        "This function creates a dictionary that keeps track of each feature's probability of each value in its support, both conditioned on the website being a phishing website and on the site being legitimate. Each entry in this data structure can be accessed as \n",
        "\n",
        "```\n",
        "probs_given_y[X_i][x][y]\n",
        "```\n",
        "Which would yield the conditional probability $P(X_i=x | Y=y)$ for any feature $X_i$, any value of that feature $x$, and any value of $y$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "e3lIcZyop-3K"
      },
      "outputs": [],
      "source": [
        "def calc_p_y(data):\n",
        "    tot_phish = 0\n",
        "    for i in data:\n",
        "        if i[\"Result\"] == 1:\n",
        "            tot_phish += 1\n",
        "    return tot_phish / len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC_8ncL_hC7F"
      },
      "source": [
        "For our ultimate prediction, we need the values of $P(Y=1)$ and $P(Y=-1)$. Because $Y$ is binary, we only need to calculate $P(Y=1)$ because $P(Y=-1) = 1- P(Y=1)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "GfAvw1DRp2rg"
      },
      "outputs": [],
      "source": [
        "def train(X, data):\n",
        "    probs_given_y = calc_probs_given_y(X, data)\n",
        "    p_y = calc_p_y(data)\n",
        "\n",
        "    return (probs_given_y, p_y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rPDU64DhaBx"
      },
      "source": [
        "This function uses the supplied data to calculate all of the conditional probabilities for the given features, as well as the probability of websites being phishing sites. This comprises all of the necessary data for our classification model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RXsnftWHqaig"
      },
      "outputs": [],
      "source": [
        "def prediction(site, X, probs_given_y, p_y):\n",
        "    X_y_neg1 = log(1 - p_y)\n",
        "    X_y_1 = log(p_y)\n",
        "\n",
        "    for X_i in X:\n",
        "        X_y_neg1 += log(probs_given_y[X_i][str(site[X_i])][0])\n",
        "        X_y_1 += log(probs_given_y[X_i][str(site[X_i])][1])\n",
        "\n",
        "    return X_y_1 > X_y_neg1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1k9OKgCJh_7e"
      },
      "source": [
        "Here's where we actually implement naive Bayes! For each site, we want to calculate whether the likelihood of it being a phishing site or a legitimate site is greater, and use that judgment to classify our given site. To do so, we need to find\n",
        " $\\text{argmax}_{y\\in\\{-1, 1\\}} P(X_1, X_2, ..., X_n | Y=y)P(Y=y)$, where $X$ is the vector of all of a site's features $X_i$. To calculate that value, we make the naive Bayes assumption that all of the features are conditionally independent on $Y$, which is most likely not actually the case, but makes computation much easier. Given that assumption, we can simply calculate $\\text{argmax}_{y\\in\\{-1, 1\\}} \\Pi_{i=1}^n P(X_i| Y=y)P(Y=y)$. Then, because logarithms are monotonically increasing, we can make this calculation easier on our computer's numerical storage by instead calculating the equivalent $\\text{argmax}_{y\\in\\{-1, 1\\}} log(P(Y=y)) + \\Sigma_{i=1}^n log(P(X_i| Y=y))$. After calculating these sums, we return 1, concluding that the site is a phishing website if $P(X | Y=1)P(Y=1) > P(X | Y=-1)P(Y=-1)$, and 0 otherwise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "l2rvY5yVqtcw"
      },
      "outputs": [],
      "source": [
        "def test(X, train_data, test_data):\n",
        "    correctly_phish = 0\n",
        "    correctly_not_phish = 0\n",
        "    predicted_phish = 0\n",
        "    actual_phish = 0\n",
        "\n",
        "    probs_given_y, p_y = train(X, train_data)\n",
        "\n",
        "    for site in test_data:\n",
        "        phishPredicted = prediction(site, X, probs_given_y, p_y)\n",
        "        if site[\"Result\"] == 1:\n",
        "            actual_phish += 1\n",
        "        if phishPredicted:\n",
        "            predicted_phish += 1\n",
        "        if phishPredicted and site[\"Result\"] == 1:\n",
        "            correctly_phish += 1\n",
        "        if not phishPredicted and site[\"Result\"] == -1:\n",
        "            correctly_not_phish += 1\n",
        "\n",
        "    print(\"Accuracy: {}\".format((correctly_phish + correctly_not_phish) / len(test_data)))\n",
        "    print(\"Precision: {}\".format(correctly_phish / predicted_phish))\n",
        "    print(\"Recall: {}\".format(correctly_phish / actual_phish))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhaxa_EFmYHt"
      },
      "source": [
        "This function first trains the naive Bayes classifier using the training data, and then predicts the status of each website using that model, while counting up which classifications it got correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfn_zrTRq9Xf",
        "outputId": "91abc211-960c-4497-9406-55eb70accdfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test results using most impactful top 5 features\n",
            "Accuracy: 0.9291314837153196\n",
            "Precision: 0.9233997901364114\n",
            "Recall: 0.9518658734451054\n"
          ]
        }
      ],
      "source": [
        "print(\"Test results using most impactful top {} features\".format(n_features))\n",
        "test(most_impact, data_train, data_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x2xe4-l0nzx4"
      },
      "source": [
        "These results are fantastic! Accuracy is the percentage of sites that were correctly classified, precision is a measure of the accuracy of positive predictions, and recall is a measure of the completeness of positive predictions. According to these numbers (if you left all the variables as their defaults) this model would classify 93% of websites correctly, and would correctly flag 95% of phishing websites. Best of all, the only data we needed to perform these predictions was $\\Sigma_{i=1}^5 |support(X_i)| \\cdot |support(Y)| = 5 \\cdot 3 \\cdot 2 = 30$ entries in our feature probability data, plus one more for p(y), totalling only 31 values stored to achieve this accuracy (note: for all features in the top 5, $|support(X_i)| = 3$ and because $Y$ is binary, $|support(Y)|$ = 2)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I_r0G8u6QRUV",
        "outputId": "d2ca7d02-f563-443d-dda1-0aeb61842d52"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'Request_URL': {'0': [0.00029129041654529564, 0.00023207240659085636], '1': [0.4614040198077483, 0.7017869575307496], '-1': [0.5385959801922516, 0.2982130424692504]}, 'web_traffic': {'0': [0.35217011360326245, 0.1413320956138315], '1': [0.304398485289834, 0.6992341610582502], '-1': [0.34372269152344886, 0.15966581573450916]}, 'Prefix_Suffix': {'0': [0.00029129041654529564, 0.00023207240659085636], '1': [0.00029129041654529564, 0.23044789974472035], '-1': [0.9997087095834547, 0.7695521002552796]}, 'SSLfinal_State': {'0': [0.23070200990387416, 0.0030169412856811324], '1': [0.14564520827264782, 0.9099559062427477], '-1': [0.6239440722400234, 0.08725922487816198]}, 'URL_of_Anchor': {'0': [0.3108068744538305, 0.6289162218612206], '1': [0.030002912904165454, 0.3641216059410536], '-1': [0.6594815030585494, 0.007194244604316547]}, 'p_y': 0.5566037735849056}\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "def save_model(probs_given_y, p_y):\n",
        "  model = probs_given_y\n",
        "  model[\"p_y\"] = p_y\n",
        "\n",
        "  with open(\"model.txt\", \"w\") as fp:\n",
        "    json.dump(model, fp)\n",
        "\n",
        "def load_model():\n",
        "  with open(\"model.txt\", \"r\") as fp:\n",
        "    model = json.load(fp)\n",
        "\n",
        "  return model\n",
        "\n",
        "probs_given_y, p_y = train(most_impact, data_train)\n",
        "save_model(probs_given_y, p_y)\n",
        "model = load_model()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5gyHLDFY_ub"
      },
      "source": [
        "Here's the entire tiny model in all of its glory! I've exported it here as json, because a great application of this model would be to make a simple JavaScript webapp for classifying websites. It's pretty cool that we can classify phishing websites with 93% accuracy using only these 31 values!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gOf1pYd4RL9",
        "outputId": "2e39443c-d0f6-442e-d961-d8527e7bbe64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "# Make sure you have fields here for each top feature selected! Otherwise, you'll run into errors.\n",
        "real_example = {\n",
        "    \"Request_URL\":-1, \n",
        "    \"web_traffic\":1, \n",
        "    \"Prefix_Suffix\":1, \n",
        "    \"SSLfinal_State\":0, \n",
        "    \"URL_of_Anchor\":1\n",
        "}\n",
        "\n",
        "probs_given_y, p_y = train(most_impact, data_train)\n",
        "print(prediction(real_example, most_impact, probs_given_y, p_y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRuMR_SspN8r"
      },
      "source": [
        "This *real_example* data is from a real phishing website linked in a phishing text I got earlier this year as part of a scam of Sam's Club customers, and it classifies it correctly! Feel free to mess around with values and see how it impacts the predicton."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JdF5bl0j8fM"
      },
      "source": [
        "Thanks for reading! I think it's awesome that we can leverage foundational probabilistic concepts to be so effective at performing a classification that could greatly reduce the tangible damage of phishing attacks."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
